#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sat Nov 25 14:34:38 2017

@author: salvatore

Learn linear regresion with bold driver
 
Original equation
y = 8.6*x1 + 0.0*x2 + 3.33

Bold Driver:
        After each iteration, if the error function decreases, you can increase 
    learning rate by 10-5%. But if it increases undo the last weight change and 
    decrease the learning rate sharply, typically 50%.
    
"""


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

F = np.array([[1,1,1,11.9], [1,2,2,20.5], [1,3,3,29.1], [1,11,4,37.7], [1,2,5,46.3], [1,1,6,54.9] ])
X= F[:,0:3]
Y = np.array([F[:,3]]).T
B = np.array([[0]]*len(X.T))

def funX(B,x):
    R = x.dot(B)
    return R

def Loss(B,x,y):
    Loss = np.sum((y - funX(B,x))**2)
    return Loss

def LinGD(i,alph,x,y):
    Btr = np.array([[0]]*len(X.T)) #Inicialization
    n=0
    ALV = []
    #alph = a
    while n<i:
        n = n+1
        DLoss = x.T.dot(y - funX(Btr,x))
        Val= (Loss(Btr,x,y) - Loss(Btr + alph*DLoss,x,y))
        #print (Val)
        if Val <=0:
            alph= alph*0.5
        else:
                alph = alph*1.1
        Bn = Btr + alph*DLoss
        A = Loss(Btr,x,y)
        ALV.append(A)    
        Btr = Bn
        print(alph)
        #print(A)
    else:
        plt.plot(ALV)
        plt.title("Loss vs iteration")
        plt.xlabel("iteration")
        plt.ylabel("Squared Error")
        plt.show()
        return np.round(Btr,2)
    


